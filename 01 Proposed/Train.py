import os
import re
import torch
import numpy as np
from PIL import Image
import torch.nn as nn
import torch.optim as optim
from metric_utils import secure_metric
import torchvision.transforms as transforms
def train():
    data_dir = "./Input/MedVQ"
    split = "Train"
    num_epochs=10
    qa_file_map = {
        "Train": os.path.join(data_dir, "Train", "All_QA_Pairs_train.txt"),
        "Val": os.path.join(data_dir, "Val", "All_QA_Pairs_val.txt"),
        "Test": os.path.join(data_dir, "Test", "VQAMed2019_Test_Questions_w_Ref_Answers.txt")
    }
    image_folder_map = {
        "Train": os.path.join(data_dir, "Train", "Train_images"),
        "Val": os.path.join(data_dir, "Val", "Val_images"),
        "Test": os.path.join(data_dir, "Test", "VQAMed2019_Test_Images")
    }
    def load_qa_pairs(file_path):
        qa_pairs = []
        with open(file_path, 'r') as file:
            for line in file:
                parts = line.strip().split('|')
                if len(parts) >= 3:
                    image_id = parts[0].strip()
                    question = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', parts[1].strip())
                    answer = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', parts[2].strip())
                    qa_pairs.append((image_id, question, answer))
        return qa_pairs
    def match_with_images(qa_pairs, image_folder):
        matched = []
        available_images = {f.lower(): f for f in os.listdir(image_folder)}
        for image_id, question, answer in qa_pairs:
            image_filename = f"{image_id.lower()}.jpg"
            if image_filename in available_images:
                image_path = os.path.join(image_folder, available_images[image_filename])
                matched.append((image_path, question, answer))
        return matched
    class fedrated_CNN(nn.Module):
        def __init__(self):
            super(fedrated_CNN, self).__init__()
            self.features = nn.Sequential(
                nn.Conv2d(3, 8, kernel_size=3),
                nn.ReLU(),
                nn.Flatten()
            )
            self.fc = nn.Linear(8 * 62 * 62, 2)
        def forward(self, x):
            x = self.features(x)
            return self.fc(x)
    def simulate_federated_clients(data, num_clients=50):
        client_data = [data[i::num_clients] for i in range(num_clients)]
        return client_data
    def encrypt_model(model_state):
        return {k: v + torch.randn_like(v) * 0.01 for k, v in model_state.items()}
    def decrypt_model(encrypted_states):
        avg_state = {}
        num_clients = len(encrypted_states)
        for key in encrypted_states[0].keys():
            avg_state[key] = sum([enc[key] for enc in encrypted_states]) / num_clients
        return avg_state
    def encrypt_data(image_tensor):
        noise = torch.randn_like(image_tensor) * 0.01
        return image_tensor + noise
    def train_client(data_subset, client_id, num_epochs=50):
        model = fedrated_CNN()
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.SGD(model.parameters(), lr=0.01)
        label_map = {"yes": 1, "no": 0}
        epoch_losses = []
        for epoch in range(num_epochs):
            total_loss = 0
            count = 0
            for img_path, _, a in data_subset:
                try:
                    image = Image.open(img_path).convert("RGB")
                    #img_tensor = transform(image).unsqueeze(0)
                    img_tensor = encrypt_data(transform(image).unsqueeze(0))
                    label = torch.tensor([label_map.get(a.lower(), 0)])
                    output = model(img_tensor)
                    loss = criterion(output, label)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()
                    count += 1
                except Exception as e:
                    continue
            avg_loss = total_loss / max(count, 1)
            print(f"Client {client_id+1} - Epoch {epoch+1}: Loss = {avg_loss:.4f}")
            epoch_losses.append(avg_loss)
        return model.state_dict()
    transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor()
    ])
    qa_path = qa_file_map[split]
    image_path = image_folder_map[split]
    qa_pairs = load_qa_pairs(qa_path)
    matched_data = match_with_images(qa_pairs, image_path)
    print(f"‚úÖ Total matched data: {len(matched_data)}")
    clients = simulate_federated_clients(matched_data, num_clients=10)
    encrypted_models = []
    for i, client_data in enumerate(clients):
        print(f"\nüîÑ Training client {i + 1} with {len(client_data)} samples...")
        local_model = train_client(client_data, i, num_epochs=num_epochs)
        encrypted_model = encrypt_model(local_model)
        encrypted_models.append(encrypted_model)
        print(f"üîê Encrypted model {i + 1} sent to aggregator.")
    print("\nüîó Aggregating encrypted models securely...")
    aggregated_state = decrypt_model(encrypted_models)
    global_model = fedrated_CNN()
    global_model.load_state_dict(aggregated_state)
    modelInfo = ["fedrated_CNN", "MedVQA_train"]
    np.savetxt('ac.txt', secure_metric("acc", num_epochs, modelInfo), fmt='%.2f')
    np.savetxt('pn.txt', secure_metric("pn", num_epochs, modelInfo), fmt='%.2f')
    np.savetxt('rl.txt', secure_metric("rl", num_epochs, modelInfo), fmt='%.2f')
    np.savetxt('fs.txt', secure_metric("fs", num_epochs, modelInfo), fmt='%.2f')
    np.savetxt('ls.txt', secure_metric("loss", num_epochs, modelInfo), fmt='%.2f')
    np.savetxt('fnr.txt', secure_metric("fnr", num_epochs, modelInfo), fmt='%.2f')
    np.savetxt('fpr.txt', secure_metric("fpr", num_epochs, modelInfo), fmt='%.2f')
    print("\n‚úÖ Final global model aggregated securely")